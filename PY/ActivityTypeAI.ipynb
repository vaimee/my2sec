{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f417ce",
   "metadata": {},
   "source": [
    "# ActivityTypeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458c93f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timezone, timedelta, datetime\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\"\"\"nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\"\"\"\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df5845",
   "metadata": {},
   "source": [
    "Load all the events without Activity Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d47309e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_graph': 'http://www.vaimee.it/my2sec/defuser@vaimee.it',\n",
       " 'username_literal': 'defuser',\n",
       " 'nodeid': 'b292',\n",
       " 'event_type': 'http://www.vaimee.it/ontology/sw#windowEvent',\n",
       " 'app': 'GitHubDesktop.exe',\n",
       " 'title': 'GitHub Desktop',\n",
       " 'datetimestamp': '2022-11-16T14:31:40.105000+00:00',\n",
       " 'activity_type': 'http://www.vaimee.it/ontology/sw#developing',\n",
       " 'duration': '0',\n",
       " 'task': 'none'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('events.txt')\n",
    "data = json.load(f)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314c704",
   "metadata": {},
   "source": [
    "Set the initial parameters (future step: use the ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a68da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = {\n",
    "    'node':'nodeid',\n",
    "    'user_graph':'user_graph',\n",
    "    'user':'username_literal',\n",
    "    'value':'value',\n",
    "    'datetimestamp':'datetimestamp',\n",
    "    'duration':'real_duration',\n",
    "    'not_afk':'',\n",
    "    'afk':'http://www.vaimee.it/ontology/sw#afkEvent',\n",
    "    'not_shutdown':'http://www.vaimee.it/ontology/sw#notShutdown',\n",
    "    'window_event':'http://www.vaimee.it/ontology/sw#windowEvent',\n",
    "    'window_event_columns':['app','title'],\n",
    "    'no_words':[\"firefox\", \"mozilla\", \"edge\", \"exe\", \"chrome\", \"safari\", \"opera\", \"google\", \"youtube\", \"http\", \"www\", \"https\", \"msedg\",\"com\",\"microsoft\"],\n",
    "    'languages':['english', 'italian'],\n",
    "    'csv_name_ai':'knwoledge.csv',\n",
    "    'activity_type':'activity_type'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f782e",
   "metadata": {},
   "source": [
    "## Train phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0919261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stopwords and languages\n",
    "stopwords_list = []\n",
    "for language in configurations['languages']:\n",
    "    stopwords_list.append(set(stopwords.words(language)))\n",
    "for n in configurations['no_words']:\n",
    "    stopwords_list[0].add(n)\n",
    "\n",
    "# load the english lemmatizer    \n",
    "english_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# load the classifier\n",
    "classifier = svm.SVC(C=10.0, kernel='linear', gamma='auto')\n",
    "\n",
    "\n",
    "'''\n",
    "Convert the list of events (i.e. list of jsons) to a defaultdict.\n",
    "In particular, it tries to aggregate the values with the same node-id.\n",
    "It takes the list of events, the node-id column name and the value column name.\n",
    "It return the defaultdict and the predicates found in the list of events.\n",
    "Ex. of json: \n",
    "{'user_graph': {'type': 'uri','value': 'http://www.vaimee.it/my2sec/gianluca.dituccio@vaimee.it'},\n",
    " 'username_literal': {'type': 'literal', 'value': 'ditucspa'},\n",
    " 'nodeid': {'type': 'bnode', 'value': 'b0'}}\n",
    "'''\n",
    "def graph2dict(ontos, nodeid=configurations['node']):\n",
    "    try:\n",
    "        predicates = []\n",
    "        dict_of_values = defaultdict(list)\n",
    "        for i in ontos:\n",
    "            for y in list(i.keys()):\n",
    "                if y == nodeid:\n",
    "                    continue\n",
    "                dict_of_values[i[nodeid]].append((y,i[y]))\n",
    "                if y not in predicates:\n",
    "                    predicates.append(y)\n",
    "        \n",
    "        return dict_of_values, predicates\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return {}, []\n",
    "\n",
    "'''\n",
    "Convert the defaultdict into a pandas dataframe.\n",
    "It takes the defaultdict, the node-id column name, the value column name and the datetimestamp column name.\n",
    "It return the pandas dataframe based on the defaultdict.\n",
    "'''\n",
    "def dict2pandas(dict_of_events, \n",
    "                nodeid=configurations['node'], \n",
    "                datetimestamp=configurations['datetimestamp']):\n",
    "    tmp = []\n",
    "    for j in browse(pd.DataFrame(dict_of_events.items(), columns=[nodeid, 'value'])['value']):\n",
    "        key = [z[0] for z in browse(j)]\n",
    "        value = [z[1] for z in browse(j)]\n",
    "        tmp.append(dict(zip(key,value)))\n",
    "    tmp = DropDuplicates_and_sort(pd.DataFrame(tmp), datetimestamp, drop_duplicates=True)\n",
    "    \n",
    "    # try to adjust the datatimestamp if it is in a wrong form\n",
    "    dates = tmp[datetimestamp].apply(lambda x: x[:x.index('+')]+'.0'+x[x.index('+'):] if not '.' in x else x).apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%f%z'))\n",
    "    tmp = tmp.drop(columns=datetimestamp)\n",
    "    tmp[datetimestamp] = dates\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def browse(element):\n",
    "    for i in element:\n",
    "        yield i\n",
    "\n",
    "'''\n",
    "Sort the values of the pandas dataframe by the column and drop the duplicates.\n",
    "It takes the dataframe to sort, the column used to sort (or a list of columns) and True if drop duplicates.\n",
    "It returns the pandas sorted and without duplicates (if the flag is activated).\n",
    "'''        \n",
    "def DropDuplicates_and_sort(dataframe, column_to_sort, drop_duplicates=False):\n",
    "    tmp = dataframe.copy()\n",
    "    if drop_duplicates:\n",
    "        tmp = tmp.drop_duplicates()\n",
    "    return tmp.sort_values(by=column_to_sort).reset_index().drop(columns='index')\n",
    "\n",
    "'''\n",
    "Resample the Not Shutdown events every 5 minutes. Then, create the events when a Not Shutdown event isn't in the dataset.\n",
    "It takes the dataframe, the not_shutdown columns name and the datetimestamp name.\n",
    "It returns the dataframe with only the not shutdown events.\n",
    "'''\n",
    "def resample_not_shutdown(dataframe, \n",
    "                          not_shutdown=configurations['not_shutdown'], \n",
    "                          datetimestamp=configurations['datetimestamp']):\n",
    "    try:\n",
    "        df = pd.DataFrame(columns=dataframe.columns)\n",
    "        events = list(dataframe[dataframe.event_type==not_shutdown][datetimestamp].values)\n",
    "        if not events: return df\n",
    "        events.append(events[-1] + np.timedelta64(5,'m'))\n",
    "        shutdown = [\"T\"]*(len(events)-1)\n",
    "        shutdown.append(None)\n",
    "        tmp = pd.DataFrame(data={\"Timestamp\":events, \"Not_Shutdown\":shutdown}, dtype='datetime64[ns, UTC]').set_index('Timestamp').resample('5min', label='right').first()\n",
    "        shutdown_false = tmp[tmp.Not_Shutdown!=\"T\"]\n",
    "        last = shutdown_false.index[0]\n",
    "        df = pd.concat([df, pd.DataFrame({datetimestamp:[last]})])\n",
    "        for i in range(1, shutdown_false.shape[0]):\n",
    "            if not last+timedelta(minutes=5) == shutdown_false.index[i]:\n",
    "                df = pd.concat([df, pd.DataFrame({datetimestamp:[shutdown_false.index[i]]})])\n",
    "                pass\n",
    "            last = shutdown_false.index[i]\n",
    "        return df\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return pd.DataFrame(columns=dataframe.columns)\n",
    "\n",
    "'''\n",
    "Add the duration to the events, using the Not Shutdown Events. It simply compute the difference between the \n",
    "event with timestamp i and the event with the timestamp i+1. The Not Shutdown prevents eventually turn-off of the application or PC.\n",
    "It takes the list of events (i.e. the list of jsons) and all the column names\n",
    "'''\n",
    "def AddDuration(json_events, \n",
    "                user_graph=configurations['user_graph'], \n",
    "                username_literal=configurations['user'], \n",
    "                nodeid=configurations['node'], \n",
    "                datetimestamp=configurations['datetimestamp'], \n",
    "                duration=configurations['duration'], \n",
    "                not_afk=configurations['not_afk'], \n",
    "                afk=configurations['afk'], \n",
    "                not_shutdow=configurations['not_shutdown']):\n",
    "    try:\n",
    "        if not json_events: raise Exception('ERROR DURING JSON PARSE: no events in the json.')\n",
    "        dict_events, predicates = graph2dict(json_events)\n",
    "        df = dict2pandas(dict_events)\n",
    "        \n",
    "        \n",
    "        final = pd.DataFrame(columns=df.columns)\n",
    "        tmp = df.copy()\n",
    "        tmp = pd.concat([tmp, resample_not_shutdown(tmp, not_shutdow, datetimestamp)])\n",
    "        tmp = DropDuplicates_and_sort(tmp, datetimestamp)\n",
    "        durations = []\n",
    "        for i in range(tmp.shape[0]):\n",
    "            try:\n",
    "                current_dt = tmp.iloc[i][datetimestamp]\n",
    "                next_dt = tmp.iloc[i+1][datetimestamp]\n",
    "                durations.append((next_dt - current_dt).total_seconds())\n",
    "            except:\n",
    "                durations.append(0)\n",
    "        tmp[duration]=durations.copy()\n",
    "        tmp = tmp.dropna(subset=[user_graph])\n",
    "        final = pd.concat([final, tmp])\n",
    "        \n",
    "        return final\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return []\n",
    "\n",
    "'''\n",
    "Remove the events with a low duration. First, all the events are grouped by apps and titles and the duration is summed.\n",
    "Then, the threshold is computed as 10% of the total duration (at most 15 minutes).\n",
    "Finally, the dynamic threshold is computed on the previous threshold (i.e. cut all the events which their duration is above the threshold).\n",
    "It takes the dataframe, column name and if cut the element below the threshold (higher=True) or above the threshold (higher=False).\n",
    "It returns the dataframe with only the events above/below the threshold.\n",
    "'''\n",
    "def RemoveLowDuration(dataframe, \n",
    "                      datetimestamp=configurations['datetimestamp'], \n",
    "                      duration=configurations['duration'], \n",
    "                      events_columns=configurations['window_event_columns'], \n",
    "                      higher=True):\n",
    "    threshold_minutes = round(dataframe[duration].sum()/60*0.1,2)\n",
    "    if threshold_minutes > 15:\n",
    "        threshold_minutes=15\n",
    "    print('10% Threshold on total duration: {0}m'.format(threshold_minutes))\n",
    "    tmp = dataframe.groupby(by=events_columns).sum().reset_index()  \n",
    "    duration_threshold = 0\n",
    "    for i in range(10,1000,2):\n",
    "        tmp2 = tmp[(tmp[duration]<i)].reset_index()\n",
    "        if tmp2[duration].sum()/60 > threshold_minutes:\n",
    "            duration_threshold = i\n",
    "            print(\"Total minutes under Threshold: {0}m\".format(round(tmp[(tmp[duration]<i)].reset_index()[duration].sum()/60,2)))\n",
    "            break\n",
    "    print(\"Threshold: {0}s\".format(duration_threshold))\n",
    "    if higher: tmp = tmp[tmp[duration]>duration_threshold]\n",
    "    else: tmp = tmp[tmp[duration]<duration_threshold]\n",
    "    df = pd.merge(dataframe, tmp, on=events_columns, how='left')\n",
    "    df = df[df[duration+'_y'].notna()]\n",
    "    df[duration]=df[duration+'_x']\n",
    "    df = df.drop(columns=[duration+'_y', duration+'_x'])\n",
    "    return df\n",
    "\n",
    "    \n",
    "'''\n",
    "Get the training events (at most 10 events per day).\n",
    "The function takes the list of events (i.e. a list of jsons).\n",
    "The function returns a list of jsons containing only the training events.\n",
    "'''\n",
    "def GetTrainActivityEvents(json_events,\n",
    "                           datetimestamp=configurations['datetimestamp'],\n",
    "                           windowEvent=configurations['window_event'],\n",
    "                           duration=configurations['duration'], \n",
    "                           events_columns=configurations['window_event_columns']):\n",
    "    try:\n",
    "        if not json_events: raise Exception('ERROR DURING JSON PARSE: no events in the json.')\n",
    "        df = DropDuplicates_and_sort(AddDuration(json_events), datetimestamp, drop_duplicates=True)\n",
    "        df = df.copy()\n",
    "        df = df[df.event_type==windowEvent]\n",
    "        \n",
    "        candidates = RemoveLowDuration(df, datetimestamp, duration, events_columns, higher=True)\n",
    "        candidates = DropDuplicates_and_sort(candidates, datetimestamp)\n",
    "        candidates = candidates.drop_duplicates(subset=events_columns, keep='last')\n",
    "        \n",
    "        train_size = 1\n",
    "        max_events_day = 10\n",
    "        # se ci sono meno di 20 eventi, ne chiede 5\n",
    "        if candidates.shape[0] < 20: train_size = round(5 / candidates.shape[0],1)\n",
    "        else: \n",
    "            #altrimenti ne chiede il 10%\n",
    "            train_size = 0.1\n",
    "            # se il 10% supera i 10 eventi, fa in modo che il train size sia tarato per chiederne massimo 10\n",
    "            if train_size * candidates.shape[0]>max_events_day: train_size = round(max_events_day / candidates.shape[0],1)\n",
    "            \n",
    "        # se ci sono meno di 5 eventi, ne chiede la metà\n",
    "        if train_size>0.9: train_size=0.5\n",
    "        if candidates.shape[0]==3: train_size = 0.7\n",
    "        print('Train size: ', train_size)\n",
    "        \n",
    "        train, test = train_test_split(candidates, train_size=train_size)\n",
    "        print('Train shape: {0}, Test shape: {1}'.format(train.shape, test.shape))\n",
    "        train = train.astype(\"string\")\n",
    "        train[datetimestamp] = train[datetimestamp].apply(lambda x: x.replace(\" \", \"T\"))\n",
    "        \n",
    "        \n",
    "        return train.to_dict('records')\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f84b916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10% Threshold on total duration: 15m\n",
      "Total minutes under Threshold: 15.29m\n",
      "Threshold: 36s\n",
      "Train size:  0.1\n",
      "Train shape: (7, 10), Test shape: (65, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_graph': 'http://www.vaimee.it/my2sec/defuser@vaimee.it',\n",
       " 'username_literal': 'defuser',\n",
       " 'event_type': 'http://www.vaimee.it/ontology/sw#windowEvent',\n",
       " 'app': 'chrome.exe',\n",
       " 'title': 'Dynamic Linked Data & Web of Things - University of Bologna - Google Chrome',\n",
       " 'activity_type': 'http://www.vaimee.it/ontology/sw#researching',\n",
       " 'duration': '226.098',\n",
       " 'task': 'none',\n",
       " 'datetimestamp': '2022-11-21T15:33:45.338000+00:00',\n",
       " 'real_duration': '227.248'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetTrainActivityEvents(data)[0] # events to show to the user (note: in future version there won't be the Activity Type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fc5a3",
   "metadata": {},
   "source": [
    "## Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f07c8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(dataframe, train=False):\n",
    "    tmp = []\n",
    "    target = []\n",
    "    tmp2 = dataframe.copy()\n",
    "    for i in range(dataframe.shape[0]):\n",
    "        #if \"google search\" in dataframe.iloc[i][\"title\"].lower(): continue\n",
    "        #if \"cerca con google\" in dataframe.iloc[i][\"title\"].lower(): continue\n",
    "        #if \"nuova scheda - google chrome\" in dataframe.iloc[i][\"title\"].lower(): continue\n",
    "        # creation of the string\n",
    "        original_text = str(dataframe.iloc[i][\"app\"] +' ') +\" \" +str(dataframe.iloc[i][\"title\"])\n",
    "\n",
    "        # remove non alphatext\n",
    "        text = re.sub('[\\W]', ' ', original_text) # to find all the alphabet letter\n",
    "\n",
    "        # text lower\n",
    "        text = text.lower()\n",
    "\n",
    "        # word tokenization\n",
    "        word_tokens = word_tokenize(text)\n",
    "\n",
    "        # removing digits\n",
    "        word_tokens = [w for w in word_tokens if not w.isnumeric() and len(w)>1]\n",
    "\n",
    "        # stopwords\n",
    "        for stop in stopwords_list:\n",
    "            word_tokens = [w for w in word_tokens if not w in stop]\n",
    "        text = ' '.join(word_tokens)\n",
    "\n",
    "        # stem\n",
    "        #text = ' '.join([english_stemmer.stem(w) for w in word_tokenize(text)])\n",
    "\n",
    "        # lemma\n",
    "        text = ' '.join([english_lemmatizer.lemmatize(w) for w in word_tokenize(text)])\n",
    "\n",
    "        # save data\n",
    "        if train:\n",
    "            #data = {\"X\":[original_text],\"X_clean\":[text],\"Y\":[dataframe.iloc[i][\"activity_type\"].split(\"#\")[1]], 'datetimestamp':[dataframe.iloc[i]['datetimestamp']]}\n",
    "            tmp.append(text)\n",
    "            target.append(dataframe.iloc[i][\"activity_type\"])\n",
    "        else:\n",
    "            #data = {\"X\":[original_text],\"X_clean\":[text], 'datetimestamp':[dataframe.iloc[i]['datetimestamp']]}   \n",
    "            tmp.append(text)\n",
    "    if target:\n",
    "        tmp2['y']=target\n",
    "    tmp2['X_clean']=tmp\n",
    "    return tmp2\n",
    "\n",
    "\n",
    "def GetTestActivityEvents(json_events, \n",
    "                          duration_name='real_duration', \n",
    "                          windowEvent_name = 'http://www.vaimee.it/ontology/sw#windowEvent', \n",
    "                          user_graph=configurations['user_graph'],\n",
    "                          knwoledge=configurations['csv_name_ai'],\n",
    "                          duration=configurations['duration'],\n",
    "                          datetimestamp=configurations['datetimestamp'],\n",
    "                          events_columns=configurations['window_event_columns'],\n",
    "                          activity_type=configurations['activity_type']):\n",
    "    try:\n",
    "        cv = TfidfVectorizer(ngram_range=(1, 1))\n",
    "        train = AddDuration(json_events['train_events'])\n",
    "        test = AddDuration(json_events['events'])\n",
    "        \n",
    "\n",
    "        # useless events   ---> magari appendere con un altro metodo????\n",
    "        useless_test = test[test.event_type!=windowEvent_name]\n",
    "        \n",
    "        \n",
    "        # store useful events\n",
    "        \n",
    "        test = test[test.event_type==windowEvent_name]\n",
    "        train = train[train.event_type==windowEvent_name]\n",
    "        \n",
    "        \n",
    "        user = train.user_graph.iloc[0]\n",
    "        \n",
    "\n",
    "        # load knowledge\n",
    "        if CheckCSV(knwoledge):\n",
    "            csv = pd.read_csv(knwoledge)\n",
    "            csv = csv[(csv.user_graph==user)&(csv.event_type==windowEvent_name)]\n",
    "            train = pd.concat([train, csv])\n",
    "\n",
    "\n",
    "        # OTHER events\n",
    "        other_events = RemoveLowDuration(test, higher=False)\n",
    "        other_events = DropDuplicates_and_sort(other_events, datetimestamp)\n",
    "        #other_events = other_events.drop_duplicates(subset=events_columns, keep='last')\n",
    "        other_events['predicted']=windowEvent_name.split('#')[0]+'#Other'\n",
    "        \n",
    "        # VALID events\n",
    "        test = RemoveLowDuration(test, higher=True)\n",
    "        test = DropDuplicates_and_sort(test, datetimestamp)\n",
    "\n",
    "        # if there's only 1 activity\n",
    "        if len(list(set(train.activity_type)))==1:\n",
    "            activity = list(set(train.activity_type))[0]\n",
    "            test['predicted']=activity\n",
    "\n",
    "        else:\n",
    "            # preprocessing and TF-IDF\n",
    "            train = apply_preprocessing(train, train=True)\n",
    "            test = apply_preprocessing(test)\n",
    "            cv.fit(pd.concat([test['X_clean'], train['X_clean']]))\n",
    "            X_train = cv.transform(train['X_clean']).toarray()\n",
    "            y_train = train['y']\n",
    "            X_test = cv.transform(test['X_clean']).toarray()\n",
    "            classifier.fit(X_train, y_train)\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            test['predicted']=y_pred\n",
    "            test = test.drop(columns='X_clean')\n",
    "        test = pd.concat([test, other_events])\n",
    "        test = pd.concat([test, useless_test]).fillna('None')\n",
    "        test = test.astype(\"string\")\n",
    "        test[datetimestamp] = test[datetimestamp].apply(lambda x: x.replace(\" \", \"T\"))\n",
    "        \n",
    "        return test.to_dict('records')\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return []\n",
    "\n",
    "        \n",
    "def CheckCSV(name_of_file):  \n",
    "    return True if os.path.isfile(name_of_file) else False\n",
    "\n",
    "def CreateCSV(json_file):\n",
    "    try:\n",
    "        dict_events, predicates = graph2dict(json_file)\n",
    "        csv = dict2pandas(dict_events)\n",
    "        if CheckCSV(configurations['csv_name_ai']):\n",
    "            csv = pd.concat([csv, pd.read_csv(configurations['csv_name_ai'])])\n",
    "        csv.to_csv(configurations['csv_name_ai'], index=False)\n",
    "        print('csv created successfully')\n",
    "        return True\n",
    "    except Exception as ex: \n",
    "        print(ex)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854640dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv created successfully\n",
      "10% Threshold on total duration: 15m\n",
      "Total minutes under Threshold: 15.29m\n",
      "Threshold: 36s\n",
      "10% Threshold on total duration: 15m\n",
      "Total minutes under Threshold: 15.29m\n",
      "Threshold: 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_graph': 'http://www.vaimee.it/my2sec/defuser@vaimee.it',\n",
       " 'username_literal': 'defuser',\n",
       " 'event_type': 'http://www.vaimee.it/ontology/sw#windowEvent',\n",
       " 'app': 'powershell.exe',\n",
       " 'title': 'Windows PowerShell',\n",
       " 'activity_type': 'http://www.vaimee.it/ontology/sw#testing',\n",
       " 'duration': '0',\n",
       " 'task': 'none',\n",
       " 'datetimestamp': '2022-11-15T16:25:25.169000+00:00',\n",
       " 'real_duration': '2.197',\n",
       " 'predicted': 'http://www.vaimee.it/ontology/sw#testing'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('train_test_events.txt')\n",
    "data = json.load(f)\n",
    "CreateCSV(data['train_events']) \n",
    "GetTestActivityEvents(data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4708286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
